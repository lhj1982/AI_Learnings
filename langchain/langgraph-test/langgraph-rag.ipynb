{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a771ef06-e5d9-4c6a-8a9e-aa715cb6a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.5.0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc5bac2c-3899-467c-9db3-5e3685795e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "# llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class RetriverScore(BaseModel):\n",
    "    \"\"\"Result of PDF reader\"\"\"\n",
    "    score: str = Field(description=\"The score of the retriever\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ").with_structured_output(RetriverScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e092c13-0e68-477e-a53c-d151ba5ece8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Set embeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "def read_pdfs_and_split(): \n",
    "  filename = \"/Users/james/projects/learnings/AI_Learnings/langchain/langgraph-test/data_sources/2025q1-alphabet-earnings-release.pdf\"\n",
    "  loader = PyPDFLoader(file_path=filename)\n",
    "  docs = []\n",
    "  docs = loader.load()\n",
    "\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=2000,\n",
    "      chunk_overlap=30,\n",
    "      add_start_index=True)\n",
    "\n",
    "  all_splits = text_splitter.split_documents(docs)\n",
    "  return all_splits\n",
    "\n",
    "all_splits = read_pdfs_and_split()\n",
    "\n",
    "# Add to vectorstore\n",
    "\"\"\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "  documents=all_splits,\n",
    "  collection_name=\"rag-chroma\",\n",
    "  embedding=embd,\n",
    ")\n",
    "\"\"\"\n",
    "vectorstore = InMemoryVectorStore.from_documents(all_splits, embd)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "300a56c6-e001-4193-86d2-ac26a7da1d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RetriverScore' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     23\u001b[39m test_web_search_2 = llm.invoke(\n\u001b[32m     24\u001b[39m     [SystemMessage(content=router_instructions)]\n\u001b[32m     25\u001b[39m     + [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhere is google headerquater\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m test_vector_store = llm.invoke(\n\u001b[32m     28\u001b[39m     [SystemMessage(content=router_instructions)]\n\u001b[32m     29\u001b[39m     + [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mGive me Google First Quarter 2025 Results\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_web_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m),\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(test_web_search_2.content),\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(test_vector_store.content),\n\u001b[32m     35\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/learnings/AI_Learnings/langchain/langgraph-test/.venv/lib/python3.11/site-packages/pydantic/main.py:891\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    888\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'RetriverScore' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to earning reports for different companies.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Test router\n",
    "test_web_search = llm.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"What's google revenu in Q1?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "test_web_search_2 = llm.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"Where is google headerquater\")]\n",
    ")\n",
    "test_vector_store = llm.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"Give me Google First Quarter 2025 Results\")]\n",
    ")\n",
    "print(\n",
    "    print(test_web_search.content),\n",
    "    print(test_web_search_2.content),\n",
    "    print(test_vector_store.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46c219d6-61e0-4232-8ff4-43c4acc5082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return RetrieverScore object, that score is 'yes' or 'no' to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"Google\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "    document=doc_txt, question=question\n",
    ")\n",
    "result = llm.invoke(\n",
    "    [SystemMessage(content=doc_grader_instructions)]\n",
    "    + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b46d6a1d-4b4d-4fb7-9d96-16a091497636",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, SystemMessage\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresponse_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetriverScore, RouteResponse, LLMResponse, DocGraderResponse\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretriever_rag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retrieve_docs\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Setup module logger\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END\n",
    "\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str  # User question\n",
    "    generation: str  # LLM generation\n",
    "    agent: str  # Binary decision to run agent\n",
    "    max_retries: int  # Max number of retries for answer generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    documents: List[str]  # List of retrieved documents\n",
    "\n",
    "def getLLM(outputSchema): \n",
    "    \"\"\"\n",
    "    Get LLM instance based on the output schema\n",
    "    \"\"\"\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "# local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "# llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from .response_models import RetriverScore, RouteResponse, LLMResponse, DocGraderResponse\n",
    "\n",
    "from .retriever_rag import retrieve_docs\n",
    "\n",
    "\n",
    "# Setup module logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prompt\n",
    "router_instructions = \"\"\"\n",
    "You are a professional equity research analyst. Analyze the stock by a given stock name or code based on its latest quarterly earnings, valuation metrics (P/E, P/S, EV/EBITDA), competitive positioning, and current macroeconomic trends. Include the following in your report:\n",
    "\n",
    "1. Company Overview – brief business summary and key revenue streams\n",
    "\n",
    "2. Recent Financial Performance – revenue, EPS, margins (QoQ and YoY)\n",
    "\n",
    "3. Valuation Comparison \n",
    "\n",
    "4. Technical Analysis – key support/resistance levels, moving averages\n",
    "\n",
    "5. Catalysts & Risks – what might drive the stock price up or down in the next 6–12 months\n",
    "\n",
    "6. Buy/Hold/Sell Recommendation – with a price target and rationale\n",
    "\n",
    "If vectorstore contains documents related to the question, use them to answer the question.\n",
    "\n",
    "If the vectorstore does not contain documents related to the question, use web search to answer the question.\n",
    "\n",
    "Return RouteResponse response include one key, datasource, that is 'agent' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getLLM(outputModel: Optional[BaseModel] = None):\n",
    "    \"\"\"\n",
    "    Get LLM instance based on the output schema\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "        # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "        # base_url=\"...\",\n",
    "        # organization=\"...\",\n",
    "        # other params...\n",
    "    ).with_structured_output(outputModel)\n",
    "    return llm\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "#    docs_txt = format_docs(documents)\n",
    "#    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "#    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    generation = \"dummy generation\"\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    # Doc grader instructions\n",
    "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "\n",
    "    # Grader prompt\n",
    "    doc_grader_prompt = \"\"\"\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n \n",
    "    Here is the user question: \\n\\n {question}. \n",
    "    \n",
    "    This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "    \n",
    "    Return DocGraderResponse response, score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    agent = \"No\"\n",
    "    for d in documents:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "            document=d.page_content, question=question\n",
    "        )\n",
    "        result = getLLM(DocGraderResponse).invoke(\n",
    "            [SystemMessage(content=doc_grader_instructions)]\n",
    "            + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = result[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            agent = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"agent\": agent}\n",
    "\n",
    "\n",
    "def call_model(state: GraphState):\n",
    "    \"\"\"\n",
    "    Call the LLM model to generate an answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    messages = state[\"question\"]\n",
    "    response = getLLM(LLMResponse).invoke(messages)\n",
    "    return { \"generation\": response }\n",
    "\n",
    "\n",
    "### Edges\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    \n",
    "    \n",
    "    route_question = getLLM(RouteResponse).invoke(\n",
    "        [SystemMessage(content=router_instructions)]\n",
    "        + [HumanMessage(content=state[\"question\"])]\n",
    "    )\n",
    "    source = route_question[\"datasource\"]\n",
    "    if source == \"websearch\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def run_workflow():\n",
    "    \"\"\"\n",
    "    Run the workflow\n",
    "    \"\"\"\n",
    "    workflow = StateGraph(GraphState)\n",
    "\n",
    "\n",
    "    # Define the nodes\n",
    "    workflow.add_node(\"run_agent\", call_model)  # web search\n",
    "    workflow.add_node(\"retrieve\", retrieve_docs)  # retrieve\n",
    "    workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "    workflow.add_node(\"generate\", generate)  # generate\n",
    "\n",
    "    # Build graph\n",
    "    workflow.set_conditional_entry_point(\n",
    "        route_question,\n",
    "        {\n",
    "            \"agent\": \"run_agent\",\n",
    "            \"vectorstore\": \"retrieve\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"run_agent\", END)\n",
    "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"agent\": \"run_agent\",\n",
    "            \"generate\": \"generate\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\n",
    "        \"generate\",\n",
    "        END\n",
    "    )\n",
    "\n",
    "    # Compile\n",
    "    graph = workflow.compile()\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n",
    "\n",
    "run_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f1d2f-634d-442f-89b5-c85baea2ab05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
